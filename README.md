# N-Grams

## Problem Description
The n-grams model is used to generate sentences based on the probability of seeing a particular sequence of n-grams in the corpus. This model also has uses outside of creating the next sentence, such as predicting the likelyhood of a translated sentence in machine translation or finding the likelyhood of different sequences of words.

## Instructions
To use this program, run this command in the terminal:

`python ngram.py n m text_file1.txt text_file2.txt ...`

- n: the number of grams to look back on to generate text
  -  for example if n = 5, then while generating the sentence, the model will look back on the previous 4 grams to pick the next gram
- m: the number of sentences to generate
- text_file1.txt text_file2.txt ... : each .txt file you want to train your model off of. for each file, you must give the relative path to it from the ngram.py file

## The Algorithm


1. The program reads the system arguments and ensure that the user gave an n, m and at least 1 .txt file
2. All of the contents in the .txt files are read into a list where each one is cleaned first by:
  1.   Replace all sequences of newline characters with a single space
  2.  Expand all concatinated words into their parts. ex: I'm -> I am, wouldn't -> would not
  3. All punctuation is either removed or replaced with the \<end\> tag
  4.  Replace \<end\> tags that that were part of words with the corect punctuation. ex: Ms \<end\> -> Ms.
  5.  Remove the rest of the punctuation and special characters I don't care about
  6.  Lowercase everything
  7. Put \<start\> tags after every \<end\> tag and cap off the documents with \<start\> and \<end\> tags
3. Create n tables for each value in the range [1, n]
4. Each table is generated by sliding a window of whatever size cooresponds to the table that is being generated. Ex: trigrams table: sliding window of size 3. Then a dictionary is generated that maps each sequence to it's frequency using a Counter object.
5. Once all the tables are generated, each sentence is generated where each one starts with "\<start\> ". 
6. To generate the next word:
  1.   If a unigram model is being used a word is packed at random, with it's chances of being chossen being weighted by it's frequency.
  2.  For all other models, the table to use is chosen by looking at the length of the current word and picking the largest table that can accomodate the previous n-1 grams. ex: if n = 5, "I like cheese" would use the n = 4 table or the sentence: "I do not like to dance" would use the n = 5 table
  3. The keys of the chosen table are then filtered down by which ones start with the last n-1 grams. ex: for n = 5, the sentence "I like cheese" would filter the keys on the 4-gram table to ones that start with "I like cheese" or the sentence "I do not like to dance" would filter the keys down to ones that start with "not like to dance".
  4.  The next word is chosen from this filtered keys list by entering the keys and cooresponding frequencies into random.choices. The last word in the chosen key is outputed as the next word in the sentence.
7. If the next choosen word is \<start\>, then the program tries again, else it appends the next word onto the sentence and stops if the next word is the \<stop\> tag
8. The new sentence has it's tags removed and the sentence is printed. This process is repeated untill all of the sentences are generated.

# Dataset

[State of the union corpus](https://www.kaggle.com/datasets/rtatman/state-of-the-union-corpus-1989-2017)
