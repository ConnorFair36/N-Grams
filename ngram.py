# Author: Connor Fair
# Class:  CMSC 437 Intro to Natural Language Processing
# Date:   2-22-26

# This program is my implementation of an n-grams model
#   The n-grams model is used to generate sentences based on the probability of seeing a particular sequence of n-grams
#   in the corpus. This model also has uses outside of creating the next sentence, such as predicting the likelyhood of
#   a translated sentence in machine translation or finding the likelyhood of different sequences of words.

# To use this program, run this command in the terminal:
#  python ngram.py n m text_file1.txt text_file2.txt ...
#   in the command above:
#   n: the number of grams to look back on to generate text
#     for example if n = 5, then while generating the sentence, the model will look back on the previous 4 grams to pick
#     the next gram
#   m: the number of sentences to generate
#   text_file1.txt text_file2.txt ...: each .txt file you want to train your model off of. for each file, you must give the
#     relative path to it from the ngram.py file

# Description of the algorithm:
#  1. The program reads the system arguments and ensure that the user gave an n, m and at least 1 .txt file
#  2. All of the contents in the .txt files are read into a list where each one is cleaned first by:
#    I.   Replace all sequences of newline characters with a single space
#    II.  Expand all concatinated words into their parts. ex: I'm -> I am, wouldn't -> would not
#    III. All punctuation is either removed or replaced with the <end> tag
#    IV.  Replace <end> tags that that were part of words with the corect punctuation. ex: Ms <end> -> Ms.
#    V.   Remove the rest of the punctuation and special characters I don't care about
#    VI.  Lowercase everything
#    VII. Put <start> tags after every <end> tag and cap off the documents with <start> and <end> tags
#  3. Create n tables for each value in the range [1, n]
#  4. Each table is generated by sliding a window of whatever size cooresponds to the table that is being generated.
#      Ex: trigrams table: sliding window of size 3. Then a dictionary is generated that maps each sequence to it's frequency
#      using a Counter object.
#  5. Once all the tables are generated, each sentence is generated where each one starts with "<start> ". 
#  6. To generate the next word:
#    I.   If a unigram model is being used a word is packed at random, with it's chances of being chossen being weighted
#           by it's frequency.
#    II.  For all other models, the table to use is chosen by looking at the length of the current word and picking the 
#           largest table that can accomodate the previous n-1 grams. ex: if n = 5, "I like cheese" would use the n = 4 table
#           or the sentence: "I do not like to dance" would use the n = 5 table
#    III. The keys of the chosen table are then filtered down by which ones start with the last n-1 grams. ex: for n = 5,
#           the sentence "I like cheese" would filter the keys on the 4-gram table to ones that start with "I like cheese"
#           or the sentence "I do not like to dance" would filter the keys down to ones that start with "not like to dance".
#    IV.  The next word is chosen from this filtered keys list by entering the keys and cooresponding frequencies into 
#           random.choices. The last word in the chosen key is outputed as the next word in the sentence.
#  7. If the next choosen word is <start>, then the program tries again, else it appends the next word onto the sentence and 
#      stops if the next word is the <stop> tag
#  8. The new sentence has it's tags removed and the sentence is printed. This process is repeated untill all of the sentences
#      are generated.

import sys
import re
from collections import Counter
import random

def verify_args(arguments: list[str]) -> tuple:
    if len(arguments) < 3:
        print("Your input must be in the format: ngram.py n m file1.txt file2.txt ...")
        raise ValueError
    n = int(arguments[0])
    m = int(arguments[1])
    files = arguments[2:]
    return n, m, files

def clean_document(document: str) -> str:
    """Cleans the document for n-grams processing."""
    # replace all \n sequences with " "
    document = re.sub(r"\n+", " ", document)
    # expand concatinated words
    document = document.replace("I'm", "I am")
    document = document.replace("'re", " are")
    document = document.replace("let's", "let us")
    document = document.replace("'s", " is")
    document = document.replace("'ve", " have")
    document = document.replace("'d", " did")
    document = document.replace("'ll", " will")
    document = document.replace("n't", " not")
    # replace " ... ... " with " "
    document = document.replace(" ... ... ", " ")
    document = document.replace("... ... ", " ")
    # replace " ..." with <end>
    document = document.replace(" ...", " <end>")
    # replace punctuation at the end of each sentence with <end> tag
    document = re.sub(r"[.!?][^0-9]", " <end> ", document)
    # put the periord for Mr. Ms. and Mrs. back if needed
    document = document.replace("Mr <end>",  "Mr.")
    document = document.replace("Ms <end>",  "Ms.")
    document = document.replace("Mrs <end>", "Mrs.")
    # remove all extra punctuation
    document = re.sub(r"[,'\"()]", "", document)
    # remove capitalization
    document = document.lower()
    # make sure the last characters in the document are an <end> tag
    if document[-6:] != " <end>":
        # remove the period at the end if there is one
        if document[-1] == ".":
            document = document[:-1]
        # make the last word an end tag
        document += " <end>"
    # add <start> to the beginning of each sentence
    document = document.replace(" <end> ", " <end> <start> ")
    # add <start> to the beginning of the document
    document = "<start> " + document
    
    return document

def create_tables(n: int, documents: list[str]) -> list:
    """Creates tables for all values <= n."""
    all_docs = " ".join(documents)
    tables = []
    for i in range(1,n+1):
        tables.append(create_table(i, all_docs))
    return tables

def create_table(n: int,  all_docs: str) -> dict:
    """Creates a table for n based on all_docs."""
    all_words = all_docs.split(" ")
    # a sliding window that concatinates words into n-sequences to be processed
    if n > 1:
        all_words = [" ".join(all_words[i:i+n]) for i in range(len(all_words) - n)]

    n_gram_frequencies = dict(Counter(all_words))
    return n_gram_frequencies

def generate_sentence(tables: list[dict]) -> str:
    """Creates a single sentence."""
    sentence = "<start>"
    next_word = ""
    while next_word != "<end>":
        next_word = get_next_word(tables, sentence)
        # reroll word choice if <start> was chosen
        if next_word == "<start>":
            continue
        sentence += " " + next_word
    # clean up the sentence before printing
    # remove start and end tags
    sentence = sentence[8:-6]
    sentence += "."
    return sentence

def get_next_word(tables: list[dict], sentence: str) -> str:
    """Gets the next word based on the current sentence."""
    # unigrams don't care about context, so just grab a word
    n = len(tables)
    if n == 1:
        uni_table = tables[0]
        return random.choices(list(uni_table.keys()), weights=list(uni_table.values()))[0]
    else:
        # gets the previous n-1 grams fro the sentence
        sentence_words = sentence.split(" ")[-n+1:]
        table = tables[len(sentence_words)]
        # filter down the table keys to just the ones that start with our n-1 grams
        prev_grams = " ".join(sentence_words) + " "
        filtered_keys = [key for key in table.keys() if key.startswith(prev_grams)]
        #print(prev_grams)
        weights = [table[key] for key in filtered_keys]
        # pick the next n words based on the previous n-1 words weighted by their frequencies
        next_sequence = random.choices(filtered_keys, weights=weights)[0]
        # return the last word
        return next_sequence.split(" ")[-1]

def main(arguments: list):
    n, m, files = verify_args(arguments)
    documents = []
    for file in files:
        with open(file=file) as f:
            documents.append(f.read())
    # clean each text file
    documents = [clean_document(doc) for doc in documents]
    tables = create_tables(n, documents)
    # print a sentence for each m
    for s in range(m):
        print(generate_sentence(tables))


if __name__ == "__main__":
    main(sys.argv[1:])

